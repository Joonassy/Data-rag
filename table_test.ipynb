{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ef56ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd2053db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonas.syrjanen\\Documents\\Data rag\\data\\warehouse\n"
     ]
    }
   ],
   "source": [
    "warehouse_path = os.path.abspath(\"data/warehouse\")\n",
    "print(warehouse_path)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"tests\") \\\n",
    "    .config(\"spark.hadoop.hadoop.native.lib\", \"false\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", warehouse_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def test_table_and_column_comments():\n",
    "    tables_df = spark.sql(\"SHOW TABLES IN local.bronze\")\\\n",
    "    .withColumn(\"fullTableName\", F.concat_ws(\".\", F.col(\"nameSpace\"), F.col(\"tableName\")))\\\n",
    "    .select(\"fullTableName\")\n",
    "\n",
    "\n",
    "    tables = [row[\"fullTableName\"] for row in tables_df.collect()]\n",
    "    \n",
    "    for table in tables:\n",
    "        full_table = f\"local.{table}\"\n",
    "        \n",
    "        # Check for table comment\n",
    "        table_info = spark.sql(f\"DESCRIBE TABLE EXTENDED {full_table}\").collect()\n",
    "        comment_line = [row for row in table_info if row.col_name == \"Comment\"]\n",
    "        assert comment_line and comment_line[0].data_type.strip(), f\"Missing table comment for {full_table}\"\n",
    "\n",
    "        # Check for column comments\n",
    "        table_column_info = spark.sql(f\"DESCRIBE TABLE {full_table}\").collect()\n",
    "        column_names = [row.col_name for row in table_column_info if row.col_name and not row.col_name.startswith(\"#\")]\n",
    "        for row in table_info:\n",
    "            if row.data_type != \"\" and row.col_name in column_names:\n",
    "                assert row.comment and row.comment.strip(), f\"Missing column comment for {full_table}.{row.col_name}\"\n",
    "\n",
    "    print(\"✅ All tables and columns have comments.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5263a086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All tables and columns have comments.\n"
     ]
    }
   ],
   "source": [
    "test_table_and_column_comments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
